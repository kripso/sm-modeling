{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from modeling_classes import JointNERAndREModel, JointNERAndREDataset\n",
    "import training_v2\n",
    "import utils\n",
    "from utils import Config\n",
    "from typing import Dict\n",
    "from sklearn.utils import class_weight\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR = os.path.dirname(os.path.abspath(\"!pwd\"))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LABELS_TO_IDS, IDS_TO_LABELS = utils.load_labels()\n",
    "RELATIONS_TO_IDS, IDS_TO_RELATIONS = utils.load_relations()\n",
    "SWEEP_CONFIG = utils.load_config(Config.SWEEP_CONFIG)\n",
    "CONFIG = utils.load_config(Config.CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels():\n",
    "    return [item for item in IDS_TO_LABELS.values()]\n",
    "def get_relations():\n",
    "    return [item for item in IDS_TO_RELATIONS.values()]\n",
    "\n",
    "\n",
    "def get_optimizer(model):\n",
    "    if wandb.config['optimizer'] == 'ADAM':\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=wandb.config[\"learning_rate\"], weight_decay=wandb.config[\"weight_decay\"])\n",
    "    if wandb.config['optimizer'] == 'ADAMW':\n",
    "        optimizer = torch.optim.AdamW(params=model.parameters(), lr=wandb.config[\"learning_rate\"], weight_decay=wandb.config[\"weight_decay\"])\n",
    "    if wandb.config['optimizer'] == 'SGD':\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(), lr=wandb.config[\"learning_rate\"], weight_decay=wandb.config[\"weight_decay\"], momentum=0.9) # noqa\n",
    "        \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=wandb.config[\"scheduler_step_size\"], gamma=wandb.config[\"scheduler_gamma\"])\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def get_datasets():\n",
    "    train_dataset = pd.read_json(f\"{CONFIG['dataset_path']}train.json\").reset_index(drop=True)\n",
    "    dev_dataset = pd.read_json(f\"{CONFIG['dataset_path']}dev.json\").reset_index(drop=True)\n",
    "    test_dataset = pd.read_json(f\"{CONFIG['dataset_path']}test.json\").reset_index(drop=True)\n",
    "\n",
    "    return train_dataset, dev_dataset, test_dataset\n",
    "\n",
    "def get_data_loaders(train_dataset, dev_dataset, test_dataset):\n",
    "    train_loader = DataLoader(JointNERAndREDataset(train_dataset, DEVICE, train=wandb.config[\"re_hack\"]), batch_size=wandb.config[\"batch_size\"], shuffle=True)\n",
    "    dev_loader = DataLoader(JointNERAndREDataset(dev_dataset, DEVICE), batch_size=wandb.config[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(JointNERAndREDataset(test_dataset, DEVICE), batch_size=wandb.config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    return train_loader, dev_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_state(model, optimizer, scheduler, metrics, model_version: str='latest', config_version: str='latest', config_overwrites: Dict[str, str]={}):\n",
    "    artifact = wandb.run.use_artifact(f'kripso/{wandb.config[\"project_name\"]}/{wandb.config[\"model\"]}:{model_version}', type='model')\n",
    "    artifact.download(f'{CURRENT_DIR}/models/')\n",
    "    artifact = wandb.run.use_artifact(f'kripso/{wandb.config[\"project_name\"]}/config:{config_version}', type='config')\n",
    "    artifact.download(f'{CURRENT_DIR}/conf/')\n",
    "    wandb.config = {**utils.load_config(Config.BACKUP), **config_overwrites}\n",
    "\n",
    "    checkpoint = torch.load(f'{CURRENT_DIR}/models/{wandb.config[\"model\"]}.pt')\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    metrics = checkpoint['metrics']\n",
    "    metrics['step'] -= 1\n",
    "\n",
    "    return model, optimizer, scheduler, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.wandb_init(CONFIG)\n",
    "def main(resume: bool=False, *args, **kwargs):\n",
    "    train_dataset, dev_dataset, test_dataset = get_datasets()\n",
    "\n",
    "    class_weights = torch.tensor([wandb.config['no_relation_weight'], *[wandb.config['relation_weight']] * 41]).to(DEVICE)\n",
    "    model = JointNERAndREModel(labels=get_labels(), relations=get_relations(), re_class_weights=class_weights).to(DEVICE)\n",
    "    optimizer, scheduler = get_optimizer(model)\n",
    "    metrics = {\"loss\": 0, \"ner_accuracy\": 0, \"ner_f1_score\": 0,\"re_accuracy\": 0, \"re_f1_score\": 0, \"index\": 1, \"step\": 0}\n",
    "\n",
    "    if resume:\n",
    "        model, optimizer, scheduler, metrics = resume_state(model, optimizer, scheduler, metrics, *args, **kwargs)\n",
    "\n",
    "    train_loader, dev_loader, test_loader = get_data_loaders(train_dataset, dev_dataset, test_dataset)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return training_v2.fit(model, optimizer, scheduler, metrics, train_loader, dev_loader, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(sweep=SWEEP_CONFIG, project=CONFIG['project_name'])\n",
    "# wandb.agent(sweep_id, function=main, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.agent('fga67i6v', project=CONFIG['project_name'], function=main, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, _, _ = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, _, _, _ = main(resume=True, config_overwrites={'epochs': 5, 'batch_size': 32})\n",
    "# model, _, _, _ = main(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = utils.string_to_list_1(\n",
    "    # \"Roland Rajcsanyi and Elon Musk own company Tesla\"\n",
    "    \"@HuggingFace is a New York company, it has employees in Paris since 1923, but it has been down today 12:30\"\n",
    ")\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    encoded = JointNERAndREDataset.tokenize(sentence, is_split=True, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "    model_out = model(encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "    flattened_predictions = torch.argmax(model_out.ner_probs.view(-1, model.num_labels), axis=1).cpu().numpy()\n",
    "    re_prediction = torch.argmax(model_out.re_probs, axis=1)\n",
    "    print(IDS_TO_RELATIONS.get(re_prediction.tolist()[0]))\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for token, mapping in zip(flattened_predictions, encoded[\"offset_mapping\"].view(-1,2).tolist()):\n",
    "        if mapping[0] == 0 and mapping[1] != 0:\n",
    "            print(f'{sentence[index]:20}  {IDS_TO_LABELS.get(token)}')\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from seqeval.metrics import classification_report, accuracy_score\n",
    "# import torchmetrics\n",
    "\n",
    "# ner_accuracy = torchmetrics.Accuracy(num_classes=model.num_labels, average=\"weighted\").to(DEVICE)\n",
    "# ner_f1_score = torchmetrics.F1Score(num_classes=model.num_labels, average=\"weighted\").to(DEVICE)\n",
    "# re_accuracy = torchmetrics.Accuracy(num_classes=model.num_relations, average=\"weighted\").to(DEVICE)\n",
    "# re_f1_score = torchmetrics.F1Score(num_classes=model.num_relations, average=\"weighted\").to(DEVICE)\n",
    "\n",
    "# ner_labels, ner_predictions, re_labels, re_predictions = training_v2.valid(model, test_loader, ner_accuracy, ner_f1_score, re_accuracy, re_f1_score, validation_loop=False)\n",
    "\n",
    "# print(accuracy_score(ner_labels, ner_predictions))\n",
    "# print(accuracy_score(re_labels, re_predictions))\n",
    "# print(classification_report([ner_labels], [ner_predictions], zero_division=False))\n",
    "# print(classification_report([[f'B-{item}' for item in re_labels]], [[f'B-{item}' for item in re_predictions]], zero_division=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from torch import nn\n",
    "\n",
    "# class_weights = torch.tensor([1 / count for count in np.bincount([0,0,0,0,1,2,2,4,4,5,3,2,1,0,0,0,1,2])])\n",
    "# print(class_weights,np.bincount([0,0,0,0,1,2,2,4,4,5,3,2,1,0,0,0,1,2]))\n",
    "# # loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# # re_loss = loss_fct(re_logits, relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# class_weights = torch.tensor([round(1 / count, 8) for count in np.bincount([RELATIONS_TO_IDS[item] for item in train_dataset['relation']])])\n",
    "# len(class_weights)\n",
    "# len([0.4,*[1.0]*41])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "414129bc5f9fadc6aae6e1926404349d54c672fb6156a1a65104540d3f3f309d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
